---
---

@inproceedings{whitehouse2023lora,
  title={Parameter-Efficient Multilingual Summarisation: An Empirical Study},
  author={Chenxi Whitehouse and Fantine Huot and Jasmijn Bastings and Mostafa Dehghani and Chu-Cheng Lin and Mirella Lapata},
  year={2024},
  abbr={NAACL},
  arxiv={2311.08572},
  url={https://arxiv.org/abs/2311.08572},
  pdf={https://arxiv.org/pdf/2311.08572.pdf},
  abstract={With the increasing prevalence of Large Language Models, traditional full fine-tuning approaches face growing challenges, especially in memory-intensive tasks. This paper investigates the potential of Parameter-Efficient Fine-Tuning, focusing on Low-Rank Adaptation (LoRA), for complex and under-explored multilingual summarisation tasks. We conduct an extensive study across different data availability scenarios, including full-data, low-data, and cross-lingual transfer, leveraging models of different sizes. Our findings reveal that LoRA is competitive with full fine-tuning when trained with full data, and excels in low-data scenarios and cross-lingual transfer. Additionally, we investigate effective strategies for few-shot cross-lingual transfer, finding that continued LoRA tuning achieves the best performance compared to both full fine-tuning and dynamic composition of language-specific LoRA modules.},
  selected={true}
}

@inproceedings{whitehouse-etal-2023-llm,
  title={LLM-powered Data Augmentation for Enhanced Cross-lingual Performance},
  author={Chenxi Whitehouse and Monojit Choudhury and Alham Fikri Aji},
  year={2023},
  abbr={EMNLP},
  arxiv={2305.14288},
  url={https://aclanthology.org/2023.emnlp-main.44},
  pdf={https://aclanthology.org/2023.emnlp-main.44.pdf},
  code={https://github.com/mbzuai-nlp/Gen-X},
  poster={papers/LLM/LLM_poster.pdf},
  slides={papers/LLM/LLM_slides.pdf},
  abstract={This paper explores the potential of leveraging Large Language Models (LLMs) for data augmentation in multilingual commonsense reasoning datasets where the available training data is extremely limited. To achieve this, we utilise several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate the effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR, using the synthesised data. We compare the performance of training with data generated in English and target languages, as well as translated English-generated data, revealing the overall advantages of incorporating data generated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best case. Furthermore, we conduct a human evaluation by asking native speakers to assess the naturalness and logical coherence of the generated examples across different languages. The results of the evaluation indicate that LLMs such as ChatGPT and GPT-4 excel at producing natural and coherent text in most languages, however, they struggle to generate meaningful text in certain languages like Tamil. We also observe that ChatGPT falls short in generating plausible alternatives compared to the original dataset, whereas examples from GPT-4 exhibit competitive logical consistency.},
  selected={true}
}

@inproceedings{wang-etal-2024-m4,
  title={M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection},
  author={Yuxia Wang and Jonibek Mansurov and Petar Ivanov and Jinyan Su and Artem Shelmanov and Akim Tsvigun and Chenxi Whitehouse and Osama Mohammed Afzal and Tarek Mahmoud and Alham Fikri Aji and Preslav Nakov},
  year={2024},
  abbr={EACL},
  arxiv={2305.14902},
  url={https://aclanthology.org/2024.eacl-long.83/},
  pdf={https://aclanthology.org/2024.eacl-long.83.pdf},
  code={https://github.com/mbzuai-nlp/M4},
  abstract=Large language models (LLMs) have demonstrated remarkable capability to generate fluent responses to a wide variety of user queries. However, this has also raised concerns about the potential misuse of such texts in journalism, education, and academia. In this study, we strive to create automated systems that can detect machine-generated texts and pinpoint potential misuse. We first introduce a large-scale benchmark M4, which is a multi-generator, multi-domain, and multi-lingual corpus for machine-generated text detection. Through an extensive empirical study of this dataset, we show that it is challenging for detectors to generalize well on instances from unseen domains or LLMs. In such cases, detectors tend to misclassify machine-generated text as human-written. These results show that the problem is far from solved and that there is a lot of room for improvement. We believe that our dataset will enable future research towards more robust approaches to this pressing societal problem. The dataset is available at https://github.com/mbzuai-nlp/M4.},
}

@inproceedings{lin2024multitask,
  title={Multitask Multilingual Model Adaptation with Featurized Low-Rank Mixtures},
  author={Chu-Cheng Lin and Xinyi Wang and Jonathan H. Clark and Han Lu and Yun Zhu and Chenxi Whitehouse and Hongkun Yu},
  year={2024},
  abbr={Preprint},
  arxiv={2402.17934},
  url={https://arxiv.org/abs/2402.17934},
  pdf={https://arxiv.org/abs/2402.17934.pdf},
  abstract={Adapting pretrained large language models (LLMs) to various downstream tasks in tens or hundreds of human languages is computationally expensive. Parameter-efficient fine-tuning (PEFT) significantly reduces the adaptation cost, by tuning only a small amount of parameters. However, directly applying PEFT methods such as LoRA (Hu et al., 2022) on diverse dataset mixtures could lead to suboptimal performance due to limited parameter capacity and negative interference among different datasets. In this work, we propose Featurized Low-rank Mixtures (FLix), a novel PEFT method designed for effective multitask multilingual tuning. FLix associates each unique dataset feature, such as the dataset's language or task, with its own low-rank weight update parameters. By composing feature-specific parameters for each dataset, FLix can accommodate diverse dataset mixtures and generalize better to unseen datasets. Our experiments show that FLix leads to significant improvements over a variety of tasks for both supervised learning and zero-shot settings using different training data mixtures.},
}

@inproceedings{whitehouse-etal-2023-webie,
  title={WebIE: Faithful and Robust Information Extraction on the Web},
  author={Chenxi Whitehouse and Clara Vania and Alham Fikri Aji and Christos Christodoulopoulos and Andrea Pierleoni},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics},
  year={2023},
  abbr={ACL},
  arxiv={2305.14293},
  url={https://aclanthology.org/2023.acl-long.428},
  pdf={https://aclanthology.org/2023.acl-long.428.pdf},
  poster={papers/webie_acl2023/WebIE_poster.pdf},
  code={https://github.com/amazon-science/WebIE},
  abstract={Extracting structured and grounded fact triples from raw text is a fundamental task in Information Extraction (IE). Existing IE datasets are typically collected from Wikipedia articles, using hyperlinks to link entities to the Wikidata knowledge base. However, models trained only on Wikipedia have limitations when applied to web domains, which often contain noisy text or text that does not have any factual information. We present WebIE, the first large-scale, entity-linked closed IE dataset consisting of 1.6M sentences automatically collected from the English Common Crawl corpus. WebIE also includes negative examples, i.e. sentences without fact triples, to better reflect the data on the web. We annotate ~21K triples from WebIE through crowdsourcing and introduce mWebIE, a translation of the annotated set in four other languages: French, Spanish, Portuguese, and Hindi. We evaluate the in-domain, out-of-domain, and zero-shot cross-lingual performance of generative IE models and find models trained on WebIE show better generalisability. We also propose three training strategies that use entity linking as an auxiliary task. Our experiments show that adding Entity-Linking objectives improves the faithfulness of our generative IE models.},
  selected={true}
}

@inproceedings{whitehouse-etal-2023-towards,
  title={Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering},
  author={Chenxi Whitehouse and Tillman Weyde and Pranava Madhyastha},
  booktitle={Findings of the Association for Computational Linguistics: EACL 2023},
  year={2023},
  abbr={EACL},
  arxiv={2301.10799},
  url={https://aclanthology.org/2023.findings-eacl.126},
  pdf={https://aclanthology.org/2023.findings-eacl.126.pdf},
  slides={papers/vqa_eacl23/EACL.pdf},
  abstract={Providing explanations for visual question answering (VQA) has gained much attention in research. However, most existing systems use separate models for the answers and the explanations. We argue that training explanation models independently of the QA model makes the explanations less grounded and limits performance. To address this, we propose a novel multitask finetuning approach towards a Unified Model for more grounded and consistent generation of both Answers and Explanations (UMAE). To achieve this, we add artificial prompt tokens to training instances and finetune a multimodal encoder-decoder model on various VQA tasks. In our experiments, UMAE models surpass the prior SOTA answer accuracy on A-OKVQA by 10~15%, show competitive results on OK-VQA, achieve new SOTA explanation scores on A-OKVQA and VCR, and demonstrate promising out-of-domain performance on VQA-X.},
  selected={true}
}

@inproceedings{whitehouse-etal-2022-entitycs,
  title={EntityCS: Improving Zero-Shot Cross-lingual Transfer with Entity-Centric Code Switching},
  author={Chenxi Whitehouse and Fenia Christopoulou and Ignacio Iacobacci},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  year={2022},
  abbr={EMNLP},
  arxiv={2210.12540},
  url={https://aclanthology.org/2022.findings-emnlp.499},
  pdf={https://aclanthology.org/2022.findings-emnlp.499.pdf},
  code={https://github.com/huawei-noah/noah-research/tree/master/NLP/EntityCS},
  poster={papers/code_switching_emnlp22/EntityCS_poster.pdf},
  abstract={Accurate alignment between languages is fundamental for improving cross-lingual pre-trained language models (XLMs). Motivated by the natural phenomenon of code-switching (CS) in multilingual speakers, CS has been used as an effective data augmentation method that offers language alignment at word- or phrase-level, in contrast to sentence-level via parallel instances. Existing approaches either use dictionaries or parallel sentences with word-alignment to generate CS data by randomly switching words in a sentence. However, such methods can be suboptimal as dictionaries disregard semantics, and syntax might become invalid after random word switching. In this work, we propose EntityCS, a method that focuses on Entity-level Code-Switching to capture fine-grained cross-lingual semantics without corrupting syntax. We use Wikidata and the English Wikipedia to construct an entity-centric CS corpus by switching entities to their counterparts in other languages. We further propose entity-oriented masking strategies during intermediate model training on the EntityCS corpus for improving entity prediction. Evaluation of the trained models on four entity-centric downstream tasks shows consistent improvements over the baseline with a notable increase of 10% in Fact Retrieval. We release the corpus and models to assist research on code-switching and enriching XLMs with external knowledge.},
  selected={true}
}

@inproceedings{Whitehouse_Weyde_Madhyastha_Komninos_2022,
  title={Evaluation of Fake News Detection with Knowledge-Enhanced Language Models},
  author={Chenxi Whitehouse and Tillman Weyde and Pranava Madhyastha and Nikos Komninos},
  booktitle={Proceedings of the International AAAI Conference on Web and Social Media 2022},
  volume={16},
  pages={1425--1429},
  year={2022},
  abbr={AAAI-ICWSM},
  code={https://github.com/chenxwh/fake-news-detection},
  poster={papers/fake_news_icwsm22/FakeNews_poster.pdf},
  url={https://ojs.aaai.org/index.php/ICWSM/article/view/19400},
  pdf={https://ojs.aaai.org/index.php/ICWSM/article/view/19400/19172},
  slides={papers/fake_news_icwsm22/ICWSM_slides.pdf},
  arxiv={2204.00458},
  abstract={Recent advances in fake news detection have exploited the success of large-scale pre-trained language models (PLMs). The predominant state-of-the-art approaches are based on fine-tuning PLMs on labelled fake news datasets. However, large-scale PLMs are generally not trained on structured factual data and hence may not possess priors that are grounded in factually accurate knowledge. The use of existing knowledge bases (KBs) with rich human-curated factual information has thus the potential to make fake news detection more effective and robust. In this paper, we investigate the impact of knowledge integration into PLMs for fake news detection. We study several state-of-the-art approaches for knowledge integration, mostly using Wikidata as KB, on two popular fake news datasets - LIAR, a politics-based dataset, and COVID-19, a dataset of messages posted on social media relating to the COVID-19 pandemic. Our experiments show that knowledge-enhanced models can significantly improve fake news detection on LIAR where the KB is relevant and up-to-date. The mixed results on COVID-19 highlight the reliance on stylistic features and the importance of domain specific and current KBs. The code is available at https://github.com/chenxwh/fake-news-detection.},
  selected={true},
}
