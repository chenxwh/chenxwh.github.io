<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content="EHy6e4kNr9U9U6GnKatHGhjS_vE4lCoJb1678UoTJNw"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Selected Publications | Chenxi Whitehouse</title> <meta name="author" content="Chenxi Whitehouse"/> <meta name="description" content=""/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/favicon.ico"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://chenxwh.github.io/publications/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://chenxwh.github.io/"><span class="font-weight-bold">Chenxi</span> Whitehouse</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Selected Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Selected Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <br><br> <h4 class="year">2026</h4> <ol class="bibliography"></ol> <h4 class="year">2025</h4> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Preprint</abbr></div> <div id="goel2025training" class="col-sm-10"> <div class="title"><a href="https://arxiv.org/abs/2512.23707" target="_blank" rel="noopener noreferrer">Training AI Co-Scientists Using Rubric Rewards</a></div> <div class="author"> Shashwat Goel, Rishi Hazra, Dulhan Jayalath, Timon Willi, Parag Jain, William F. Shen, Ilias Leontiadis, Francesco Barbieri, Yoram Bachrach, Jonas Geiping, and <em>Chenxi Whitehouse</em> </div> <div class="periodical"> <em></em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2512.23707" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://arxiv.org/pdf/2512.23707" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> </div> <div class="abstract hidden"> <p>AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICLR</abbr></div> <div id="whitehouse2025menlo" class="col-sm-10"> <div class="title"><a href="https://arxiv.org/abs/2509.26601" target="_blank" rel="noopener noreferrer">MENLO: From Preferences to Proficiency – Evaluating and Modeling Native-like Quality Across 47 Languages</a></div> <div class="author"> <em>Chenxi Whitehouse</em>, <a href="https://scholar.google.com/citations?user=8ONXPV8AAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Sebastian Ruder</a>, Tony Lin, Oksana Kurylo, Haruka Takagi, Janice Lam, Nicolò Busetto, Denise Diaz, and <a href="https://scholar.google.com/citations?user=eEBT0BAAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Francisco Guzmán</a> </div> <div class="periodical"> <em></em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2509.26601" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://arxiv.org/pdf/2509.26601" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> </div> <div class="abstract hidden"> <p>Ensuring native-like quality of large language model (LLM) responses across many languages is challenging. To address this, we introduce MENLO, a framework that operationalizes the evaluation of native-like response quality based on audience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423 human-annotated prompt-response preference pairs covering four quality dimensions with high inter-annotator agreement in 47 language varieties. Our evaluation reveals that zero-shot LLM judges benefit significantly from pairwise evaluation and our structured annotation rubrics, yet they still underperform human annotators on our dataset. We demonstrate substantial improvements through fine-tuning with reinforcement learning, reward shaping, and multi-task learning approaches. Additionally, we show that RL-trained judges can serve as generative reward models to enhance LLMs’ multilingual proficiency, though discrepancies with human judgment remain. Our findings suggest promising directions for scalable multilingual evaluation and preference alignment. We release our dataset and evaluation framework to support further research in multilingual LLM evaluation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ICLR</abbr></div> <div id="whitehouse2025j1" class="col-sm-10"> <div class="title"><a href="https://arxiv.org/abs/2505.10320" target="_blank" rel="noopener noreferrer">J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning</a></div> <div class="author"> <em>Chenxi Whitehouse</em>, Tianlu Wang, Ping Yu, Xian Li, <a href="https://scholar.google.com/citations?user=lMkTx0EAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Jason Weston</a>, <a href="https://scholar.google.de/citations?user=fN7fYXIAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Ilia Kulikov</a>, and <a href="https://scholar.google.com/citations?user=sY5SyBgAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Swarnadeep Saha</a> </div> <div class="periodical"> <em></em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2505.10320" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://arxiv.org/pdf/2505.10320" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> </div> <div class="abstract hidden"> <p>The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think. In this work we introduce J1, a reinforcement learning approach to training such models. Our method converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards that incentivize thinking and mitigate judgment bias. In particular, our approach outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. J1 also outperforms o1-mini, and even R1 on some benchmarks, despite training a smaller model. We provide analysis and ablations comparing Pairwise-J1 vs Pointwise-J1 models, offline vs online training recipes, reward strategies, seed prompts, and variations in thought length and content. We find that our models make better judgments by learning to outline evaluation criteria, comparing against self-generated reference answers, and re-evaluating the correctness of model responses.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Preprint</abbr></div> <div id="radharapu2025calibrating" class="col-sm-10"> <div class="title"><a href="https://arxiv.org/abs/2512.22245" target="_blank" rel="noopener noreferrer">Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation</a></div> <div class="author"> Bhaktipriya Radharapu, Eshika Saxena, Kenneth Li,  <em>Chenxi Whitehouse</em>, <a href="https://scholar.google.com/citations?user=MUtbKt0AAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Adina Williams</a>, and <a href="https://scholar.google.com/citations?user=PXGsctkAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Nicola Cancedda</a> </div> <div class="periodical"> <em></em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2512.22245" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://arxiv.org/pdf/2512.22245" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> </div> <div class="abstract hidden"> <p>As LLM-based judges become integral to industry applications, obtaining well-calibrated uncertainty estimates efficiently has become critical for production deployment. However, existing techniques, such as verbalized confidence and multi-generation methods, are often either poorly calibrated or computationally expensive. We introduce linear probes trained with a Brier score-based loss to provide calibrated uncertainty estimates from reasoning judges’ hidden states, requiring no additional model training. We evaluate our approach on both objective tasks (reasoning, mathematics, factuality, coding) and subjective human preference judgments. Our results demonstrate that probes achieve superior calibration compared to existing methods with ≈10x computational savings, generalize robustly to unseen evaluation domains, and deliver higher accuracy on high-confidence predictions. However, probes produce conservative estimates that underperform on easier datasets but may benefit safety-critical deployments prioritizing low false-positive rates. Overall, our work demonstrates that interpretability-based uncertainty estimation provides a practical and scalable plug-and-play solution for LLM judges in production.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ACL</abbr></div> <div id="liu2024vista" class="col-sm-10"> <div class="title"><a href="https://aclanthology.org/2025.acl-long.310" target="_blank" rel="noopener noreferrer">What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations</a></div> <div class="author"> <a href="https://scholar.google.com/citations?user=KrSiNLwAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Dongqi Liu</a>,  <em>Chenxi Whitehouse</em>, Xi Yu, Louis Mahon, Rohit Saxena, Zheng Zhao, Yifu Qiu, <a href="https://scholar.google.co.uk/citations?user=j67B9Q4AAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Mirella Lapata</a>, and Vera Demberg </div> <div class="periodical"> <em></em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2502.08279" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://aclanthology.org/2025.acl-long.310.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> </div> <div class="abstract hidden"> <p>Transforming recorded videos into concise and accurate textual summaries is a growing challenge in multimodal learning. This paper introduces VISTA, a dataset specifically designed for video-to-text summarization in scientific domains. VISTA contains 18,599 recorded AI conference presentations paired with their corresponding paper abstracts. We benchmark the performance of state-of-the-art large models and apply a plan-based framework to better capture the structured nature of abstracts. Both human and automated evaluations confirm that explicit planning enhances summary quality and factual consistency. However, a considerable gap remains between models and human performance, highlighting the challenges of our dataset. This study aims to pave the way for future research on scientific video-to-text summarization.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ACL</abbr></div> <div id="zhu2024diffusion" class="col-sm-10"> <div class="title"><a href="https://aclanthology.org/2025.acl-long.210" target="_blank" rel="noopener noreferrer">Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models</a></div> <div class="author"> Xiaochen Zhu, Georgi Karadzhov,  <em>Chenxi Whitehouse</em>, and <a href="https://scholar.google.co.uk/citations?user=XjWnyM4AAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Andreas Vlachos</a> </div> <div class="periodical"> <em></em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2412.11333" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://aclanthology.org/2025.acl-long.210.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> </div> <div class="abstract hidden"> <p>Diffusion models have shown promise in text generation, but often struggle with generating long, coherent, and contextually accurate text. Token-level diffusion doesn’t model word-order dependencies explicitly and operates on short, fixed output windows, while passage-level diffusion struggles with learning robust representations for long-form text. To address these challenges, we propose Segment-Level Diffusion (SLD), a framework that enhances diffusion-based text generation through text segmentation, robust representation training with adversarial and contrastive learning, and improved latent-space guidance. By segmenting long-form outputs into multiple latent representations and decoding them with an autoregressive decoder, SLD simplifies diffusion predictions and improves scalability. Experiments on four datasets demonstrate that, when compared to other diffusion and autoregressive baselines SLD achieves competitive or superior fluency, coherence, and contextual compatibility in automatic and human evaluations.</p> </div> </div> </div> </li> </ol> <h4 class="year">2024</h4> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NeurIPS</abbr></div> <div id="romero2024cvqa" class="col-sm-10"> <div class="title"><a href="https://openreview.net/forum?id=E18kRXTGmV" target="_blank" rel="noopener noreferrer">CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark</a></div> <div class="author"> David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo,  ...,  <em>Chenxi Whitehouse</em>,  ..., and <a href="https://scholar.google.com/citations?user=0Cyfqv4AAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Alham Fikri Aji</a> </div> <div class="periodical"> <em font="">The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2406.05967" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://openreview.net/pdf?id=E18kRXTGmV" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> </div> <div class="abstract hidden"> <p>Visual Question Answering (VQA) is an important task in multimodal AI, which requires models to understand and reason on knowledge present in visual and textual data. However, most of the current VQA datasets and models are primarily focused on English and a few major world languages, with images that are Western-centric. While recent efforts have tried to increase the number of languages covered on VQA datasets, they still lack diversity in low-resource languages. More importantly, some datasets extend the text to other languages, either via translation or some other approaches, but usually keep the same images, resulting in narrow cultural representation. To address these limitations, we create CVQA, a new Culturally-diverse Multilingual Visual Question Answering benchmark dataset, designed to cover a rich set of languages and regions, where we engage native speakers and cultural experts in the data collection process. CVQA includes culturally-driven images and questions from across 28 countries in four continents, covering 26 languages with 11 scripts, providing a total of 9k questions. We benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and we show that the dataset is challenging for the current state-of-the-art models. This benchmark will serve as a probing evaluation suite for assessing the cultural bias of multimodal models and hopefully encourage more research efforts towards increasing cultural awareness and linguistic diversity in this field.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">FEVER</abbr></div> <div id="schlichtkrull-etal-2024-automated" class="col-sm-10"> <div class="title"><a href="https://aclanthology.org/2024.fever-1.1" target="_blank" rel="noopener noreferrer">The Automated Verification of Textual Claims (AVeriTeC) Shared Task</a></div> <div class="author"> <a href="https://scholar.google.com/citations?user=z8YvWyEAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Michael Schlichtkrull</a>, Yulong Chen,  <em>Chenxi Whitehouse</em>, Zhenyun Deng, Mubashara Akhtar, <a href="https://scholar.google.com/citations?user=dbzGY5YAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Rami Aly</a>, Zhijiang Guo, <a href="https://scholar.google.com/citations?user=oZORQtwAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Christos Christodoulopoulos</a>, Oana Cocarascu, Arpit Mittal, <a href="https://scholar.google.co.uk/citations?user=hao9RrgAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">James Thorne</a>, and <a href="https://scholar.google.co.uk/citations?user=XjWnyM4AAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Andreas Vlachos</a> </div> <div class="periodical"> <em font="">Proceedings of the Seventh Fact Extraction and VERification Workshop (FEVER)</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2410.23850" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://aclanthology.org/2024.fever-1.1.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> </div> <div class="abstract hidden"> <p>The Automated Verification of Textual Claims (AVeriTeC) shared task asks participants to retrieve evidence and predict veracity for real-world claims checked by fact-checkers. Evidence can be found either via a search engine, or via a knowledge store provided by the organisers. Submissions are evaluated using the AVeriTeC score, which considers a claim to be accurately verified if and only if both the verdict is correct and retrieved evidence is considered to meet a certain quality threshold. The shared task received 21 submissions, 18 of which surpassed our baseline. The winning team was TUDA_MAI with an AVeriTeC score of 63%. In this paper we describe the shared task, present the full results, and highlight key takeaways from the shared task.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div> <div id="lovenia2024seacrowd" class="col-sm-10"> <div class="title"><a href="https://aclanthology.org/2024.emnlp-main.296" target="_blank" rel="noopener noreferrer">SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages</a></div> <div class="author"> Holy Lovenia, Rahmad Mahendra, Salsabil Maulana Akbar,  ...,  <em>Chenxi Whitehouse</em>,  ..., and <a href="https://scholar.google.com/citations?user=w5w_WZEAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Samuel Cahyawijaya</a> </div> <div class="periodical"> <em font="">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2406.10118" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://aclanthology.org/2024.emnlp-main.296.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> </div> <div class="abstract hidden"> <p>Southeast Asia (SEA) is a region rich in linguistic diversity and cultural variety, with over 1,300 indigenous languages and a population of 671 million people. However, prevailing AI models suffer from a significant lack of representation of texts, images, and audio datasets from SEA, compromising the quality of AI models for SEA languages. Evaluating models for SEA languages is challenging due to the scarcity of high-quality datasets, compounded by the dominance of English training data, raising concerns about potential cultural misrepresentation. To address these challenges, we introduce SEACrowd, a collaborative initiative that consolidates a comprehensive resource hub that fills the resource gap by providing standardized corpora in nearly 1,000 SEA languages across three modalities. Through our SEACrowd benchmarks, we assess the quality of AI models on 36 indigenous languages across 13 tasks, offering valuable insights into the current AI landscape in SEA. Furthermore, we propose strategies to facilitate greater AI advancements, maximizing potential utility and resource equity for the future of AI in SEA.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">COLM</abbr></div> <div id="yuan2024problem" class="col-sm-10"> <div class="title"><a href="https://openreview.net/forum?id=k8KS9Ps71d" target="_blank" rel="noopener noreferrer">PRobELM: Plausibility Ranking Evaluation for Language Models</a></div> <div class="author"> Moy Yuan,  <em>Chenxi Whitehouse</em>, Eric Chamoun, <a href="https://scholar.google.com/citations?user=dbzGY5YAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Rami Aly</a>, and <a href="https://scholar.google.co.uk/citations?user=XjWnyM4AAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Andreas Vlachos</a> </div> <div class="periodical"> <em font="">First Conference on Language Modeling</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2404.03818" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://openreview.net/pdf?id=k8KS9Ps71d" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces PRobELM (Plausibility Ranking Evaluation for Language Models), a benchmark designed to assess language models’ ability to discern more plausible from less plausible scenarios through their parametric knowledge. While benchmarks such as TruthfulQA emphasise factual accuracy or truthfulness, and others such as COPA explore plausible scenarios without explicitly incorporating world knowledge, PRobELM seeks to bridge this gap by evaluating models’ capabilities to prioritise plausible scenarios that leverage world knowledge over less plausible alternatives. This design allows us to assess the potential of language models for downstream use cases such as literature-based discovery where the focus is on identifying information that is likely but not yet known. Our benchmark is constructed from a dataset curated from Wikidata edit histories, tailored to align the temporal bounds of the training data for the evaluated models. PRobELM facilitates the evaluation of language models across multiple prompting types, including statement, text completion, and question-answering. Experiments with 10 models of various sizes and architectures on the relationship between model scales, training recency, and plausibility performance, reveal that factual accuracy does not directly correlate with plausibility performance and that up-to-date training data enhances plausibility assessment across different model architectures.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NAACL</abbr></div> <div id="whitehouse2023lora" class="col-sm-10"> <div class="title"><a href="https://aclanthology.org/2024.findings-naacl.77" target="_blank" rel="noopener noreferrer">Low-Rank Adaptaion for Multilingual Summarisation: An Empirical Study</a></div> <div class="author"> <em>Chenxi Whitehouse</em>, <a href="https://scholar.google.com/citations?user=79VvQLMAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Fantine Huot</a>, <a href="https://scholar.google.com/citations?user=VG_wuYkAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Jasmijn Bastings</a>, <a href="https://scholar.google.com/citations?user=MiHOX3QAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Mostafa Dehghani</a>, <a href="https://scholar.google.com/citations?user=bgR6Gu4AAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Chu-Cheng Lin</a>, and <a href="https://scholar.google.co.uk/citations?user=j67B9Q4AAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Mirella Lapata</a> </div> <div class="periodical"> <em font="">Findings of the Association for Computational Linguistics: NAACL 2024</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2311.08572" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://aclanthology.org/2024.findings-naacl.77/.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> <a href="/assets/pdf/papers/LoRA_naacl24/NAACL-poster.pdf" class="btn btn-sm z-depth-0" role="button"><i class="far fa-image"></i> poster</a> <a href="/assets/pdf/papers/LoRA_naacl24/NAACL-slides.pdf" class="btn btn-sm z-depth-0" role="button"><i class="far fa-file-powerpoint"></i> slides</a> </div> <div class="abstract hidden"> <p>Although the advancements of pre-trained Large Language Models have significantly accelerated recent progress in NLP, their ever-increasing size poses significant challenges for conventional fine-tuning, especially in memory-intensive tasks. We investigate the potential of Parameter-Efficient Fine-Tuning, focusing on Low-Rank Adaptation (LoRA), in the domain of multilingual summarization, a task that is both challenging (due to typically long inputs), and relatively unexplored. We conduct an extensive study across different data availability scenarios, including high- and low-data settings, and cross-lingual transfer, leveraging models of different sizes. Our findings reveal that LoRA is competitive with full fine-tuning when trained with high quantities of data, and excels in low-data scenarios and cross-lingual transfer. We also study different strategies for few-shot cross-lingual transfer, finding that continued LoRA tuning outperforms full fine-tuning and the dynamic composition of language-specific LoRA modules.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EACL</abbr></div> <div id="wang-etal-2024-m4" class="col-sm-10"> <div class="title"><a href="https://aclanthology.org/2024.eacl-long.83" target="_blank" rel="noopener noreferrer">M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated Text Detection</a></div> <div class="author"> Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun,  <em>Chenxi Whitehouse</em>, Osama Mohammed Afzal, Tarek Mahmoud, <a href="https://scholar.google.com/citations?user=0Cyfqv4AAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Alham Fikri Aji</a>, and <a href="https://scholar.google.com/citations?user=DfXsKZ4AAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Preslav Nakov</a> </div> <div class="periodical"> <em font="">Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.14902" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://aclanthology.org/2024.eacl-long.83.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> <a href="https://github.com/mbzuai-nlp/M4" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="fas fa-code"></i> code</a> </div> <div class="abstract hidden"> <p>Large language models (LLMs) have demonstrated remarkable capability to generate fluent responses to a wide variety of user queries. However, this has also raised concerns about the potential misuse of such texts in journalism, education, and academia. In this study, we strive to create automated systems that can detect machine-generated texts and pinpoint potential misuse. We first introduce a large-scale benchmark M4, which is a multi-generator, multi-domain, and multi-lingual corpus for machine-generated text detection. Through an extensive empirical study of this dataset, we show that it is challenging for detectors to generalize well on instances from unseen domains or LLMs. In such cases, detectors tend to misclassify machine-generated text as human-written. These results show that the problem is far from solved and that there is a lot of room for improvement. We believe that our dataset will enable future research towards more robust approaches to this pressing societal problem. The dataset is available at https://github.com/mbzuai-nlp/M4.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Preprint</abbr></div> <div id="lin2024multitask" class="col-sm-10"> <div class="title"><a href="https://arxiv.org/abs/2402.17934" target="_blank" rel="noopener noreferrer">Inducing Generalization across Languages and Tasks using Featurized Low-Rank Mixtures</a></div> <div class="author"> <a href="https://scholar.google.com/citations?user=bgR6Gu4AAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Chu-Cheng Lin</a>, Xinyi Wang, Jonathan H. Clark, Han Lu, Yun Zhu,  <em>Chenxi Whitehouse</em>, and Hongkun Yu </div> <div class="periodical"> <em font=""></em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2402.17934" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://arxiv.org/pdf/2402.17934" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> </div> <div class="abstract hidden"> <p>Adapting pretrained large language models (LLMs) to various downstream tasks in tens or hundreds of human languages is computationally expensive. Parameter-efficient fine-tuning (PEFT) significantly reduces the adaptation cost, by tuning only a small amount of parameters. However, common PEFT methods LoRA (Hu et al., 2022) suffer from suboptimal performance on diverse dataset mixtures, due to aggressive parameter tying and negative interference among different datasets. In this work, we propose Featurized Low-rank Mixtures (FLix), a novel PEFT method designed for effective multitask multilingual adaptation. FLix associates each unique dataset feature, such as the dataset’s language or task, with its own low-rank weight update parameters. By composing feature-specific parameters for each dataset, FLix can accommodate diverse dataset mixtures and generalize better to unseen datasets. Our experiments show that FLix leads to significant improvements over a variety of tasks for both supervised learning and zero-shot settings with gains of up to 14.2 in exact match points in zero-shot semantic parsing.</p> </div> </div> </div> </li> </ol> <h4 class="year">2023</h4> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div> <div id="whitehouse-etal-2023-llm" class="col-sm-10"> <div class="title"><a href="https://aclanthology.org/2023.emnlp-main.44" target="_blank" rel="noopener noreferrer">LLM-powered Data Augmentation for Enhanced Cross-lingual Performance</a></div> <div class="author"> <em>Chenxi Whitehouse</em>, <a href="https://scholar.google.com/citations?user=WR1ImCMAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Monojit Choudhury</a>, and <a href="https://scholar.google.com/citations?user=0Cyfqv4AAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Alham Fikri Aji</a> </div> <div class="periodical"> <em font="">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.14288" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://aclanthology.org/2023.emnlp-main.44.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> <a href="https://github.com/mbzuai-nlp/Gen-X" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="fas fa-code"></i> code</a> <a href="/assets/pdf/papers/LLM/LLM_poster.pdf" class="btn btn-sm z-depth-0" role="button"><i class="far fa-image"></i> poster</a> <a href="/assets/pdf/papers/LLM/LLM_slides.pdf" class="btn btn-sm z-depth-0" role="button"><i class="far fa-file-powerpoint"></i> slides</a> </div> <div class="abstract hidden"> <p>This paper explores the potential of leveraging Large Language Models (LLMs) for data augmentation in multilingual commonsense reasoning datasets where the available training data is extremely limited. To achieve this, we utilise several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate the effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR, using the synthesised data. We compare the performance of training with data generated in English and target languages, as well as translated English-generated data, revealing the overall advantages of incorporating data generated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best case. Furthermore, we conduct a human evaluation by asking native speakers to assess the naturalness and logical coherence of the generated examples across different languages. The results of the evaluation indicate that LLMs such as ChatGPT and GPT-4 excel at producing natural and coherent text in most languages, however, they struggle to generate meaningful text in certain languages like Tamil. We also observe that ChatGPT falls short in generating plausible alternatives compared to the original dataset, whereas examples from GPT-4 exhibit competitive logical consistency.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ACL</abbr></div> <div id="whitehouse-etal-2023-webie" class="col-sm-10"> <div class="title"><a href="https://aclanthology.org/2023.acl-long.428" target="_blank" rel="noopener noreferrer">WebIE: Faithful and Robust Information Extraction on the Web</a></div> <div class="author"> <em>Chenxi Whitehouse</em>, <a href="https://scholar.google.com/citations?user=4WvFs_IAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Clara Vania</a>, <a href="https://scholar.google.com/citations?user=0Cyfqv4AAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Alham Fikri Aji</a>, <a href="https://scholar.google.com/citations?user=oZORQtwAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Christos Christodoulopoulos</a>, and <a href="https://scholar.google.co.uk/citations?user=4qU3GDIAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Andrea Pierleoni</a> </div> <div class="periodical"> <em font="">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.14293" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://aclanthology.org/2023.acl-long.428.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> <a href="https://github.com/amazon-science/WebIE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="fas fa-code"></i> code</a> <a href="/assets/pdf/papers/webie_acl2023/WebIE_poster.pdf" class="btn btn-sm z-depth-0" role="button"><i class="far fa-image"></i> poster</a> </div> <div class="abstract hidden"> <p>Extracting structured and grounded fact triples from raw text is a fundamental task in Information Extraction (IE). Existing IE datasets are typically collected from Wikipedia articles, using hyperlinks to link entities to the Wikidata knowledge base. However, models trained only on Wikipedia have limitations when applied to web domains, which often contain noisy text or text that does not have any factual information. We present WebIE, the first large-scale, entity-linked closed IE dataset consisting of 1.6M sentences automatically collected from the English Common Crawl corpus. WebIE also includes negative examples, i.e. sentences without fact triples, to better reflect the data on the web. We annotate  21K triples from WebIE through crowdsourcing and introduce mWebIE, a translation of the annotated set in four other languages: French, Spanish, Portuguese, and Hindi. We evaluate the in-domain, out-of-domain, and zero-shot cross-lingual performance of generative IE models and find models trained on WebIE show better generalisability. We also propose three training strategies that use entity linking as an auxiliary task. Our experiments show that adding Entity-Linking objectives improves the faithfulness of our generative IE models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EACL</abbr></div> <div id="whitehouse-etal-2023-towards" class="col-sm-10"> <div class="title"><a href="https://aclanthology.org/2023.findings-eacl.126" target="_blank" rel="noopener noreferrer">Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering</a></div> <div class="author"> <em>Chenxi Whitehouse</em>, <a href="https://scholar.google.co.uk/citations?user=DUgCliAAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Tillman Weyde</a>, and <a href="https://scholar.google.com/citations?user=Tarh6WoAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Pranava Madhyastha</a> </div> <div class="periodical"> <em font="">Findings of the Association for Computational Linguistics: EACL 2023</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2301.10799" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://aclanthology.org/2023.findings-eacl.126.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> <a href="/assets/pdf/papers/vqa_eacl23/EACL.pdf" class="btn btn-sm z-depth-0" role="button"><i class="far fa-file-powerpoint"></i> slides</a> </div> <div class="abstract hidden"> <p>Providing explanations for visual question answering (VQA) has gained much attention in research. However, most existing systems use separate models for the answers and the explanations. We argue that training explanation models independently of the QA model makes the explanations less grounded and limits performance. To address this, we propose a novel multitask finetuning approach towards a Unified Model for more grounded and consistent generation of both Answers and Explanations (UMAE). To achieve this, we add artificial prompt tokens to training instances and finetune a multimodal encoder-decoder model on various VQA tasks. In our experiments, UMAE models surpass the prior SOTA answer accuracy on A-OKVQA by 10 15%, show competitive results on OK-VQA, achieve new SOTA explanation scores on A-OKVQA and VCR, and demonstrate promising out-of-domain performance on VQA-X.</p> </div> </div> </div> </li> </ol> <h4 class="year">2022</h4> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div> <div id="whitehouse-etal-2022-entitycs" class="col-sm-10"> <div class="title"><a href="https://aclanthology.org/2022.findings-emnlp.499" target="_blank" rel="noopener noreferrer">EntityCS: Improving Zero-Shot Cross-lingual Transfer with Entity-Centric Code Switching</a></div> <div class="author"> <em>Chenxi Whitehouse</em>, <a href="https://scholar.google.co.uk/citations?user=nVKZdJkAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Fenia Christopoulou</a>, and <a href="https://scholar.google.it/citations?user=zOmsu9EAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Ignacio Iacobacci</a> </div> <div class="periodical"> <em font="">Findings of the Association for Computational Linguistics: EMNLP 2022</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2210.12540" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://aclanthology.org/2022.findings-emnlp.499.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> <a href="https://github.com/huawei-noah/noah-research/tree/master/NLP/EntityCS" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="fas fa-code"></i> code</a> <a href="/assets/pdf/papers/code_switching_emnlp22/EntityCS_poster.pdf" class="btn btn-sm z-depth-0" role="button"><i class="far fa-image"></i> poster</a> </div> <div class="abstract hidden"> <p>Accurate alignment between languages is fundamental for improving cross-lingual pre-trained language models (XLMs). Motivated by the natural phenomenon of code-switching (CS) in multilingual speakers, CS has been used as an effective data augmentation method that offers language alignment at word- or phrase-level, in contrast to sentence-level via parallel instances. Existing approaches either use dictionaries or parallel sentences with word-alignment to generate CS data by randomly switching words in a sentence. However, such methods can be suboptimal as dictionaries disregard semantics, and syntax might become invalid after random word switching. In this work, we propose EntityCS, a method that focuses on Entity-level Code-Switching to capture fine-grained cross-lingual semantics without corrupting syntax. We use Wikidata and the English Wikipedia to construct an entity-centric CS corpus by switching entities to their counterparts in other languages. We further propose entity-oriented masking strategies during intermediate model training on the EntityCS corpus for improving entity prediction. Evaluation of the trained models on four entity-centric downstream tasks shows consistent improvements over the baseline with a notable increase of 10% in Fact Retrieval. We release the corpus and models to assist research on code-switching and enriching XLMs with external knowledge.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">AAAI-ICWSM</abbr></div> <div id="Whitehouse_Weyde_Madhyastha_Komninos_2022" class="col-sm-10"> <div class="title"><a href="https://ojs.aaai.org/index.php/ICWSM/article/view/19400" target="_blank" rel="noopener noreferrer">Evaluation of Fake News Detection with Knowledge-Enhanced Language Models</a></div> <div class="author"> <em>Chenxi Whitehouse</em>, <a href="https://scholar.google.co.uk/citations?user=DUgCliAAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Tillman Weyde</a>, <a href="https://scholar.google.com/citations?user=Tarh6WoAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Pranava Madhyastha</a>, and <a href="https://scholar.google.com/citations?user=rVGN5hUAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Nikos Komninos</a> </div> <div class="periodical"> <em font="">Proceedings of the Sixteenth International AAAI Conference on Web and Social Media</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2204.00458" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://ojs.aaai.org/index.php/ICWSM/article/view/19400/19172" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> <a href="https://github.com/chenxwh/fake-news-detection" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="fas fa-code"></i> code</a> <a href="/assets/pdf/papers/fake_news_icwsm22/FakeNews_poster.pdf" class="btn btn-sm z-depth-0" role="button"><i class="far fa-image"></i> poster</a> <a href="/assets/pdf/papers/fake_news_icwsm22/ICWSM_slides.pdf" class="btn btn-sm z-depth-0" role="button"><i class="far fa-file-powerpoint"></i> slides</a> </div> <div class="abstract hidden"> <p>Recent advances in fake news detection have exploited the success of large-scale pre-trained language models (PLMs). The predominant state-of-the-art approaches are based on fine-tuning PLMs on labelled fake news datasets. However, large-scale PLMs are generally not trained on structured factual data and hence may not possess priors that are grounded in factually accurate knowledge. The use of existing knowledge bases (KBs) with rich human-curated factual information has thus the potential to make fake news detection more effective and robust. In this paper, we investigate the impact of knowledge integration into PLMs for fake news detection. We study several state-of-the-art approaches for knowledge integration, mostly using Wikidata as KB, on two popular fake news datasets - LIAR, a politics-based dataset, and COVID-19, a dataset of messages posted on social media relating to the COVID-19 pandemic. Our experiments show that knowledge-enhanced models can significantly improve fake news detection on LIAR where the KB is relevant and up-to-date. The mixed results on COVID-19 highlight the reliance on stylistic features and the importance of domain specific and current KBs. The code is available at https://github.com/chenxwh/fake-news-detection.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Chenxi Whitehouse. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>