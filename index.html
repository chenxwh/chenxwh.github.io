<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content="EHy6e4kNr9U9U6GnKatHGhjS_vE4lCoJb1678UoTJNw"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Chenxi Whitehouse</title> <meta name="author" content="Chenxi Whitehouse"/> <meta name="description" content=""/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/favicon.ico"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://chenxwh.github.io/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%63%68%65%6E%78%69.%77%68%69%74%65%68%6F%75%73%65@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=MxJqtPIAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/chenxwh" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/chenxwh" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/chenxi_jw" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Chenxi</span> Whitehouse </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/me-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/me-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/me-1400.webp"></source> <img src="/assets/img/me.jpg" class="img-fluid z-depth-1 rounded-circle" width="auto" height="auto" alt="me.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>I am Chenxi, a research scientist at <a href="https://ai.meta.com/research/" target="_blank" rel="noopener noreferrer">Meta</a>, focusing on Fundamental AI Research for LLMs. I am also a visiting researcher at the <a href="https://www.cst.cam.ac.uk/about" target="_blank" rel="noopener noreferrer">University of Cambridge</a>, where I previously worked as a postdoctoral research associate, collaborating with <a href="https://www.cst.cam.ac.uk/people/av308" target="_blank" rel="noopener noreferrer">Prof. Andreas Vlachos</a> on factuality in NLP.</p> <p>Before joining Meta, I was an applied research scientist at <a href="https://www.amazon.science/locations/london-and-cambridge" target="_blank" rel="noopener noreferrer">Amazon AGI</a> in Cambridge, where I focused on advancing Alexa with LLMs. I completed my PhD in knowledge-grounded NLP at <a href="https://www.city.ac.uk/" target="_blank" rel="noopener noreferrer">City, University of London</a>. Earlier, I received a Bachelorâ€™s degree in Information Engineering from <a href="http://en.xjtu.edu.cn/" target="_blank" rel="noopener noreferrer">Xiâ€™an Jiaotong University</a> and a Masterâ€™s degree in Electrical Engineering from the <a href="https://www.fau.eu/" target="_blank" rel="noopener noreferrer">University of Erlangen-NÃ¼rnberg</a> and <a href="https://www.ucl.ac.uk/" target="_blank" rel="noopener noreferrer">University College London</a>.</p> <p>My research focuses on Natural Language Understanding and Generation, Multilinguality, Knowledge Distillation, and Reasoning. During my PhD, I interned at <a href="https://www.deepmind.com/" target="_blank" rel="noopener noreferrer">Google DeepMind</a> in Amsterdam, <a href="https://amazon.jobs/en-gb/landing_pages/Cambridge" target="_blank" rel="noopener noreferrer">Amazon Alexa AI</a> in Cambridge, <a href="https://noahlab.com.hk/" target="_blank" rel="noopener noreferrer">Huawei Noahâ€™s Ark Lab</a> in London, and visited <a href="https://mbzuai.ac.ae/" target="_blank" rel="noopener noreferrer">MBZUAI</a> as a research assistant.</p> </div> <div class="news"> <br><br> <h2>News</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="min-width: 100px; white-space: nowrap;">May - 2025</th> <td> Two papers <a href="https://arxiv.org/abs/2502.08279" target="_blank" rel="noopener noreferrer">What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations</a> and <a href="https://arxiv.org/abs/2412.11333" target="_blank" rel="noopener noreferrer">Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models</a> are accepted at <a href="https://2025.aclweb.org/" target="_blank" rel="noopener noreferrer">ACL 2025</a> main conference! <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> </td> </tr> <tr> <th scope="row" style="min-width: 100px; white-space: nowrap;">Oct - 2024</th> <td> Thrilled to share that I have joined <a href="https://ai.meta.com/research/" target="_blank" rel="noopener noreferrer">Meta GenAI</a> in London as a Research Scientist! </td> </tr> <tr> <th scope="row" style="min-width: 100px; white-space: nowrap;">Jul - 2024</th> <td> Our paper <a href="https://arxiv.org/abs/2404.03818" target="_blank" rel="noopener noreferrer">PRobELM: Plausibility Ranking Evaluation for Language Models</a> is accepted in the first Conference of Language Modelling <a href="https://colmweb.org/index.html" target="_blank" rel="noopener noreferrer">COLM 2024</a>! <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> </td> </tr> <tr> <th scope="row" style="min-width: 100px; white-space: nowrap;">Jun - 2024</th> <td> Happy to share that I have joined <a href="https://www.amazon.science/locations/london-and-cambridge/" target="_blank" rel="noopener noreferrer">Amazon AGI</a> in Cambridge as an Applied Research Scientist! </td> </tr> <tr> <th scope="row" style="min-width: 100px; white-space: nowrap;">Mar - 2024</th> <td> Our paper <a href="https://aclanthology.org/2024.findings-naacl.77" target="_blank" rel="noopener noreferrer">Low-Rank Adaptation for Multilingual Summarisation: An Empirical Study</a> from my internship at <a href="https://deepmind.google/" target="_blank" rel="noopener noreferrer">Google DeepMind</a> is accepted in the findings of <a href="https://2024.naacl.org/" target="_blank" rel="noopener noreferrer">NAACL 2024</a>! <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> </td> </tr> <tr> <th scope="row" style="min-width: 100px; white-space: nowrap;">Feb - 2024</th> <td> Happy to share that I have passed my Viva! Warmest thanks to everyone who helped my PhD journey and the amazing examiners <a href="https://isabelleaugenstein.github.io/" target="_blank" rel="noopener noreferrer">Prof Isabelle Augenstein</a> and <a href="https://nikosaletras.com/" target="_blank" rel="noopener noreferrer">Prof Nikos Aletras</a>! ðŸŽ“ </td> </tr> <tr> <th scope="row" style="min-width: 100px; white-space: nowrap;">Dec - 2023</th> <td> Excited to share that I have joined <a href="https://www.cst.cam.ac.uk/" target="_blank" rel="noopener noreferrer">University of Cambridge</a> as a Postdoc researcher, working with <a href="https://www.cst.cam.ac.uk/people/av308" target="_blank" rel="noopener noreferrer">Prof Andreas Vlachos</a> on automated fact-checking! </td> </tr> <tr> <th scope="row" style="min-width: 100px; white-space: nowrap;">Oct - 2023</th> <td> Our paper <a href="https://aclanthology.org/2023.emnlp-main.44/" target="_blank" rel="noopener noreferrer">LLM-powered Data Augmentation for Enhanced Cross-lingual Performance</a> from my visit at <a href="https://mbzuai.ac.ae/" target="_blank" rel="noopener noreferrer">MBZUAI</a> is accepted in the main conference of <a href="https://2023.emnlp.org/" target="_blank" rel="noopener noreferrer">EMNLP 2023</a>! <img class="emoji" title=":tada:" alt=":tada:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png" height="20" width="20"> </td> </tr> </table> </div> </div> <div class="publications"> <h2>Selected Publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">Preprint</abbr></div> <div id="whitehouse2025j1" class="col-sm-10"> <div class="title"><a href="https://arxiv.org/abs/2505.10320" target="_blank" rel="noopener noreferrer">J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning</a></div> <div class="author"> <em>Chenxi Whitehouse</em>,Â Tianlu Wang,Â Ping Yu,Â Xian Li,Â <a href="https://scholar.google.com/citations?user=lMkTx0EAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Jason Weston</a>,Â <a href="https://scholar.google.de/citations?user=fN7fYXIAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Ilia Kulikov</a>,Â and <a href="https://scholar.google.com/citations?user=sY5SyBgAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Swarnadeep Saha</a> </div> <div class="periodical"> <em></em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2505.10320" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://arxiv.org/pdf/2505.10320" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> </div> <div class="abstract hidden"> <p>The progress of AI is bottlenecked by the quality of evaluation, and powerful LLM-as-a-Judge models have proved to be a core solution. Improved judgment ability is enabled by stronger chain-of-thought reasoning, motivating the need to find the best recipes for training such models to think. In this work we introduce J1, a reinforcement learning approach to training such models. Our method converts both verifiable and non-verifiable prompts to judgment tasks with verifiable rewards that incentivize thinking and mitigate judgment bias. In particular, our approach outperforms all other existing 8B or 70B models when trained at those sizes, including models distilled from DeepSeek-R1. J1 also outperforms o1-mini, and even R1 on some benchmarks, despite training a smaller model. We provide analysis and ablations comparing Pairwise-J1 vs Pointwise-J1 models, offline vs online training recipes, reward strategies, seed prompts, and variations in thought length and content. We find that our models make better judgments by learning to outline evaluation criteria, comparing against self-generated reference answers, and re-evaluating the correctness of model responses.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">NAACL</abbr></div> <div id="whitehouse2023lora" class="col-sm-10"> <div class="title"><a href="https://aclanthology.org/2024.findings-naacl.77" target="_blank" rel="noopener noreferrer">Low-Rank Adaptaion for Multilingual Summarisation: An Empirical Study</a></div> <div class="author"> <em>Chenxi Whitehouse</em>,Â <a href="https://scholar.google.com/citations?user=79VvQLMAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Fantine Huot</a>,Â <a href="https://scholar.google.com/citations?user=VG_wuYkAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Jasmijn Bastings</a>,Â <a href="https://scholar.google.com/citations?user=MiHOX3QAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Mostafa Dehghani</a>,Â <a href="https://scholar.google.com/citations?user=bgR6Gu4AAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Chu-Cheng Lin</a>,Â and <a href="https://scholar.google.co.uk/citations?user=j67B9Q4AAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Mirella Lapata</a> </div> <div class="periodical"> <em font>Findings of the Association for Computational Linguistics: NAACL 2024</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2311.08572" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://aclanthology.org/2024.findings-naacl.77/.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> <a href="/assets/pdf/papers/LoRA_naacl24/NAACL-poster.pdf" class="btn btn-sm z-depth-0" role="button"><i class="far fa-image"></i> poster</a> <a href="/assets/pdf/papers/LoRA_naacl24/NAACL-slides.pdf" class="btn btn-sm z-depth-0" role="button"><i class="far fa-file-powerpoint"></i> slides</a> </div> <div class="abstract hidden"> <p>Although the advancements of pre-trained Large Language Models have significantly accelerated recent progress in NLP, their ever-increasing size poses significant challenges for conventional fine-tuning, especially in memory-intensive tasks. We investigate the potential of Parameter-Efficient Fine-Tuning, focusing on Low-Rank Adaptation (LoRA), in the domain of multilingual summarization, a task that is both challenging (due to typically long inputs), and relatively unexplored. We conduct an extensive study across different data availability scenarios, including high- and low-data settings, and cross-lingual transfer, leveraging models of different sizes. Our findings reveal that LoRA is competitive with full fine-tuning when trained with high quantities of data, and excels in low-data scenarios and cross-lingual transfer. We also study different strategies for few-shot cross-lingual transfer, finding that continued LoRA tuning outperforms full fine-tuning and the dynamic composition of language-specific LoRA modules.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div> <div id="whitehouse-etal-2023-llm" class="col-sm-10"> <div class="title"><a href="https://aclanthology.org/2023.emnlp-main.44" target="_blank" rel="noopener noreferrer">LLM-powered Data Augmentation for Enhanced Cross-lingual Performance</a></div> <div class="author"> <em>Chenxi Whitehouse</em>,Â <a href="https://scholar.google.com/citations?user=WR1ImCMAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Monojit Choudhury</a>,Â and <a href="https://scholar.google.com/citations?user=0Cyfqv4AAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Alham Fikri Aji</a> </div> <div class="periodical"> <em font>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.14288" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://aclanthology.org/2023.emnlp-main.44.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> <a href="https://github.com/mbzuai-nlp/Gen-X" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="fas fa-code"></i> code</a> <a href="/assets/pdf/papers/LLM/LLM_poster.pdf" class="btn btn-sm z-depth-0" role="button"><i class="far fa-image"></i> poster</a> <a href="/assets/pdf/papers/LLM/LLM_slides.pdf" class="btn btn-sm z-depth-0" role="button"><i class="far fa-file-powerpoint"></i> slides</a> </div> <div class="abstract hidden"> <p>This paper explores the potential of leveraging Large Language Models (LLMs) for data augmentation in multilingual commonsense reasoning datasets where the available training data is extremely limited. To achieve this, we utilise several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate the effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR, using the synthesised data. We compare the performance of training with data generated in English and target languages, as well as translated English-generated data, revealing the overall advantages of incorporating data generated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best case. Furthermore, we conduct a human evaluation by asking native speakers to assess the naturalness and logical coherence of the generated examples across different languages. The results of the evaluation indicate that LLMs such as ChatGPT and GPT-4 excel at producing natural and coherent text in most languages, however, they struggle to generate meaningful text in certain languages like Tamil. We also observe that ChatGPT falls short in generating plausible alternatives compared to the original dataset, whereas examples from GPT-4 exhibit competitive logical consistency.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ACL</abbr></div> <div id="whitehouse-etal-2023-webie" class="col-sm-10"> <div class="title"><a href="https://aclanthology.org/2023.acl-long.428" target="_blank" rel="noopener noreferrer">WebIE: Faithful and Robust Information Extraction on the Web</a></div> <div class="author"> <em>Chenxi Whitehouse</em>,Â <a href="https://scholar.google.com/citations?user=4WvFs_IAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Clara Vania</a>,Â <a href="https://scholar.google.com/citations?user=0Cyfqv4AAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Alham Fikri Aji</a>,Â <a href="https://scholar.google.com/citations?user=oZORQtwAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Christos Christodoulopoulos</a>,Â and <a href="https://scholar.google.co.uk/citations?user=4qU3GDIAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Andrea Pierleoni</a> </div> <div class="periodical"> <em font>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2305.14293" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://aclanthology.org/2023.acl-long.428.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> <a href="https://github.com/amazon-science/WebIE" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="fas fa-code"></i> code</a> <a href="/assets/pdf/papers/webie_acl2023/WebIE_poster.pdf" class="btn btn-sm z-depth-0" role="button"><i class="far fa-image"></i> poster</a> </div> <div class="abstract hidden"> <p>Extracting structured and grounded fact triples from raw text is a fundamental task in Information Extraction (IE). Existing IE datasets are typically collected from Wikipedia articles, using hyperlinks to link entities to the Wikidata knowledge base. However, models trained only on Wikipedia have limitations when applied to web domains, which often contain noisy text or text that does not have any factual information. We present WebIE, the first large-scale, entity-linked closed IE dataset consisting of 1.6M sentences automatically collected from the English Common Crawl corpus. WebIE also includes negative examples, i.e. sentences without fact triples, to better reflect the data on the web. We annotate Â 21K triples from WebIE through crowdsourcing and introduce mWebIE, a translation of the annotated set in four other languages: French, Spanish, Portuguese, and Hindi. We evaluate the in-domain, out-of-domain, and zero-shot cross-lingual performance of generative IE models and find models trained on WebIE show better generalisability. We also propose three training strategies that use entity linking as an auxiliary task. Our experiments show that adding Entity-Linking objectives improves the faithfulness of our generative IE models.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EACL</abbr></div> <div id="whitehouse-etal-2023-towards" class="col-sm-10"> <div class="title"><a href="https://aclanthology.org/2023.findings-eacl.126" target="_blank" rel="noopener noreferrer">Towards a Unified Model for Generating Answers and Explanations in Visual Question Answering</a></div> <div class="author"> <em>Chenxi Whitehouse</em>,Â <a href="https://scholar.google.co.uk/citations?user=DUgCliAAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Tillman Weyde</a>,Â and <a href="https://scholar.google.com/citations?user=Tarh6WoAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Pranava Madhyastha</a> </div> <div class="periodical"> <em font>Findings of the Association for Computational Linguistics: EACL 2023</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2301.10799" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://aclanthology.org/2023.findings-eacl.126.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> <a href="/assets/pdf/papers/vqa_eacl23/EACL.pdf" class="btn btn-sm z-depth-0" role="button"><i class="far fa-file-powerpoint"></i> slides</a> </div> <div class="abstract hidden"> <p>Providing explanations for visual question answering (VQA) has gained much attention in research. However, most existing systems use separate models for the answers and the explanations. We argue that training explanation models independently of the QA model makes the explanations less grounded and limits performance. To address this, we propose a novel multitask finetuning approach towards a Unified Model for more grounded and consistent generation of both Answers and Explanations (UMAE). To achieve this, we add artificial prompt tokens to training instances and finetune a multimodal encoder-decoder model on various VQA tasks. In our experiments, UMAE models surpass the prior SOTA answer accuracy on A-OKVQA by 10Â 15%, show competitive results on OK-VQA, achieve new SOTA explanation scores on A-OKVQA and VCR, and demonstrate promising out-of-domain performance on VQA-X.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div> <div id="whitehouse-etal-2022-entitycs" class="col-sm-10"> <div class="title"><a href="https://aclanthology.org/2022.findings-emnlp.499" target="_blank" rel="noopener noreferrer">EntityCS: Improving Zero-Shot Cross-lingual Transfer with Entity-Centric Code Switching</a></div> <div class="author"> <em>Chenxi Whitehouse</em>,Â <a href="https://scholar.google.co.uk/citations?user=nVKZdJkAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Fenia Christopoulou</a>,Â and <a href="https://scholar.google.it/citations?user=zOmsu9EAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Ignacio Iacobacci</a> </div> <div class="periodical"> <em font>Findings of the Association for Computational Linguistics: EMNLP 2022</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2210.12540" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://aclanthology.org/2022.findings-emnlp.499.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> <a href="https://github.com/huawei-noah/noah-research/tree/master/NLP/EntityCS" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="fas fa-code"></i> code</a> <a href="/assets/pdf/papers/code_switching_emnlp22/EntityCS_poster.pdf" class="btn btn-sm z-depth-0" role="button"><i class="far fa-image"></i> poster</a> </div> <div class="abstract hidden"> <p>Accurate alignment between languages is fundamental for improving cross-lingual pre-trained language models (XLMs). Motivated by the natural phenomenon of code-switching (CS) in multilingual speakers, CS has been used as an effective data augmentation method that offers language alignment at word- or phrase-level, in contrast to sentence-level via parallel instances. Existing approaches either use dictionaries or parallel sentences with word-alignment to generate CS data by randomly switching words in a sentence. However, such methods can be suboptimal as dictionaries disregard semantics, and syntax might become invalid after random word switching. In this work, we propose EntityCS, a method that focuses on Entity-level Code-Switching to capture fine-grained cross-lingual semantics without corrupting syntax. We use Wikidata and the English Wikipedia to construct an entity-centric CS corpus by switching entities to their counterparts in other languages. We further propose entity-oriented masking strategies during intermediate model training on the EntityCS corpus for improving entity prediction. Evaluation of the trained models on four entity-centric downstream tasks shows consistent improvements over the baseline with a notable increase of 10% in Fact Retrieval. We release the corpus and models to assist research on code-switching and enriching XLMs with external knowledge.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">AAAI-ICWSM</abbr></div> <div id="Whitehouse_Weyde_Madhyastha_Komninos_2022" class="col-sm-10"> <div class="title"><a href="https://ojs.aaai.org/index.php/ICWSM/article/view/19400" target="_blank" rel="noopener noreferrer">Evaluation of Fake News Detection with Knowledge-Enhanced Language Models</a></div> <div class="author"> <em>Chenxi Whitehouse</em>,Â <a href="https://scholar.google.co.uk/citations?user=DUgCliAAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Tillman Weyde</a>,Â <a href="https://scholar.google.com/citations?user=Tarh6WoAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Pranava Madhyastha</a>,Â and <a href="https://scholar.google.com/citations?user=rVGN5hUAAAAJ&amp;hl=en" style="color:rgb(0,0,0)" target="_blank" rel="noopener noreferrer">Nikos Komninos</a> </div> <div class="periodical"> <em font>Proceedings of the Sixteenth International AAAI Conference on Web and Social Media</em> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2204.00458" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a href="https://ojs.aaai.org/index.php/ICWSM/article/view/19400/19172" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="far fa-file-pdf"></i> PDF</a> <a href="https://github.com/chenxwh/fake-news-detection" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer"><i class="fas fa-code"></i> code</a> <a href="/assets/pdf/papers/fake_news_icwsm22/FakeNews_poster.pdf" class="btn btn-sm z-depth-0" role="button"><i class="far fa-image"></i> poster</a> <a href="/assets/pdf/papers/fake_news_icwsm22/ICWSM_slides.pdf" class="btn btn-sm z-depth-0" role="button"><i class="far fa-file-powerpoint"></i> slides</a> </div> <div class="abstract hidden"> <p>Recent advances in fake news detection have exploited the success of large-scale pre-trained language models (PLMs). The predominant state-of-the-art approaches are based on fine-tuning PLMs on labelled fake news datasets. However, large-scale PLMs are generally not trained on structured factual data and hence may not possess priors that are grounded in factually accurate knowledge. The use of existing knowledge bases (KBs) with rich human-curated factual information has thus the potential to make fake news detection more effective and robust. In this paper, we investigate the impact of knowledge integration into PLMs for fake news detection. We study several state-of-the-art approaches for knowledge integration, mostly using Wikidata as KB, on two popular fake news datasets - LIAR, a politics-based dataset, and COVID-19, a dataset of messages posted on social media relating to the COVID-19 pandemic. Our experiments show that knowledge-enhanced models can significantly improve fake news detection on LIAR where the KB is relevant and up-to-date. The mixed results on COVID-19 highlight the reliance on stylistic features and the importance of domain specific and current KBs. The code is available at https://github.com/chenxwh/fake-news-detection.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> Â© Copyright 2025 Chenxi Whitehouse. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>